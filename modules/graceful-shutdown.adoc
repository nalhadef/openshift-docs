// Module included in the following assemblies:
//
// * backup_and_restore/graceful-cluster-shutdown.adoc

:_content-type: PROCEDURE
[id="graceful-shutdown_{context}"]
= Shutting down the cluster

You can shut down your cluster in a graceful manner so that it can be restarted at a later date.

[NOTE]
====
You can shut down a cluster until a year from the installation date and expect it to restart gracefully. After a year from the installation date, the cluster certificates expire.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have taken an etcd backup.
+
[IMPORTANT]
====
It is important to take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues when restarting the cluster.

For example, the following conditions can cause the restarted cluster to malfunction:

* etcd data corruption during shutdown
* Node failure due to hardware
* Network connectivity issues

If your cluster fails to recover, follow the steps to restore to a previous cluster state.
====

.Procedure

. If you are shutting the cluster down for an extended period, determine the date on which certificates expire.
+
[source,terminal]
----
$ oc -n openshift-kube-apiserver-operator get secret kube-apiserver-to-kubelet-signer -o jsonpath='{.metadata.annotations.auth\.openshift\.io/certificate-not-after}'
----
+
.Example output
----
2022-08-05T14:37:50Zuser@user:~ $ <1>
----
<1> To ensure that the cluster can restart gracefully, plan to restart it on or before the specified date. As the cluster restarts, the process might require you to manually approve the pending certificate signing requests (CSRs) to recover kubelet certificates.

. Mark the control plane nodes in the cluster as unschedulable. You can do this from your cloud provider's web console, or run the following loop:
+
[source,yaml]
----
$ for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}'); do echo ${node} ; oc adm cordon ${node} ; done
----
+
.Example output
[source,terminal]
----
ci-ln-mgdnf4b-72292-n547t-master-0
node/ci-ln-mgdnf4b-72292-n547t-master-0 cordoned
ci-ln-mgdnf4b-72292-n547t-master-1
node/ci-ln-mgdnf4b-72292-n547t-master-1 cordoned
ci-ln-mgdnf4b-72292-n547t-master-2
node/ci-ln-mgdnf4b-72292-n547t-master-2 cordoned
ci-ln-mgdnf4b-72292-n547t-worker-a-s7ntl
node/ci-ln-mgdnf4b-72292-n547t-worker-a-s7ntl cordoned
ci-ln-mgdnf4b-72292-n547t-worker-b-cmc9k
node/ci-ln-mgdnf4b-72292-n547t-worker-b-cmc9k cordoned
ci-ln-mgdnf4b-72292-n547t-worker-c-vcmtn
node/ci-ln-mgdnf4b-72292-n547t-worker-c-vcmtn cordoned
----

... Check the kubelet server certificate expiration date by running the following command:
+
. Evacuate the pods using the following method:
+
[source,yaml]
----
$ for node in $(oc get nodes -l node-role.kubernetes.io/worker -o jsonpath='{.items[*].metadata.name}'); do echo ${node} ; oc adm drain ${node} --delete-emptydir-data --ignore-daemonsets=true --timeout=15s ; done
----
+
. Shut down all of the nodes in the cluster. You can do this from your cloud providerâ€™s web console, or run the following loop:
+
[source,yaml]
----
$ for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}'); do oc debug node/${node} -- chroot /host shutdown -h 1 ; done
----

